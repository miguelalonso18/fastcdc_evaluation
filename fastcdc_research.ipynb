{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329ecefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24f75f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and Directories in '/home/miguel/dev/dedup/fastcdc_evaluation/data':\n",
      "['comp-videos-zip', 'textbooks', 'videos']\n"
     ]
    }
   ],
   "source": [
    "# Change This!!!\n",
    "base_windows = \"C:\\\\Users\\\\Miguel\\\\Downloads\" # windows\n",
    "base_linux = '/home/miguel/dev/dedup/fastcdc_evaluation/data' # linux\n",
    "\n",
    "os_name = platform.system()\n",
    "base_path = base_windows if os_name == \"Windows\" else base_linux\n",
    "\n",
    "folders = []\n",
    "obj = os.scandir(base_path)\n",
    "print(\"Files and Directories in '% s':\" % base_path)\n",
    "for entry in obj :\n",
    "    if entry.is_dir() or entry.is_file():\n",
    "        if not entry.name == \"memorydumps\":\n",
    "            folders.append(entry.name)\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abc1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastCDCScanResponse:\n",
    "    def __init__(self, files, chunk_min, chunk_avg, chunk_max, unique_chunks, total_data, dupe_data, dedupe_ratio, throughput, throughput_unit):\n",
    "        self.files = files\n",
    "        self.chunk_min = chunk_min\n",
    "        self.chunk_avg = chunk_avg\n",
    "        self.chunk_max = chunk_max\n",
    "        self.unique_chunks = unique_chunks\n",
    "        self.total_data = total_data\n",
    "        self.dupe_data = dupe_data\n",
    "        self.dedupe_ratio = dedupe_ratio\n",
    "        self.throughput = throughput\n",
    "        self.throughput_unit = throughput_unit\n",
    "        \n",
    "def processFastCDCScanResponse(response):\n",
    "    r = list(filter(None, re.split('\\s|\\n', response)))\n",
    "    if len(r) == 30:\n",
    "        obj = FastCDCScanResponse(r[1], r[5], r[8], r[11], r[14], r[17], r[21], r[25], r[28], r[29])\n",
    "        return obj\n",
    "    return None\n",
    "\n",
    "def processFoldersbyChunk(folders, chunk_min, chunk_avg, chunk_max):\n",
    "    responses = []\n",
    "    for folder in folders:\n",
    "        if not folder == 'memorydumps': \n",
    "            print(\"Processing Folder: \" + str(folder))\n",
    "            command = \"fastcdc scan \" + base_path + \"/\" + str(folder) + \" -r -mi \" + str(chunk_min) + \" -s \"+ str(chunk_avg) + \" -ma \" + str(chunk_max)\n",
    "            output = subprocess.check_output(command, shell=True).decode('utf-8')\n",
    "            response = processFastCDCScanResponse(output)\n",
    "            if response:\n",
    "                responses.append(response)\n",
    "    return responses\n",
    "\n",
    "def processDedup(tests):\n",
    "    o = []\n",
    "    for chunk_size in tests.keys():\n",
    "        j = []\n",
    "        for folder in tests[chunk_size]:\n",
    "            d = folder.dedupe_ratio\n",
    "            j.append(float(d))\n",
    "        o.append(j)\n",
    "    return np.array(o)\n",
    "\n",
    "def processThroughput(tests):\n",
    "    o = []\n",
    "    for chunk_size in tests.keys():\n",
    "        j = []\n",
    "        for folder in tests[chunk_size]:\n",
    "            d = folder.throughput\n",
    "            unit = folder.throughput_unit\n",
    "            if unit == 'GB/s':\n",
    "                j.append(float(d) * 1000)\n",
    "            else:\n",
    "                j.append(float(d))\n",
    "        o.append(j)\n",
    "    return np.array(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0954207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test: 1 Chunk Size: 4096\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "\n",
      "Test: 2 Chunk Size: 8192\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "\n",
      "Test: 3 Chunk Size: 12288\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "\n",
      "Test: 4 Chunk Size: 16384\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "\n",
      "Test: 5 Chunk Size: 20480\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "\n",
      "Test: 6 Chunk Size: 24576\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "\n",
      "Test: 7 Chunk Size: 28672\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "\n",
      "Test: 8 Chunk Size: 32768\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "\n",
      "Test: 9 Chunk Size: 36864\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "\n",
      "Test: 10 Chunk Size: 40960\n",
      "Processing Folder: comp-videos-zip\n",
      "Processing Folder: textbooks\n",
      "Processing Folder: videos\n",
      "[[0.02 0.01 0.  ]\n",
      " [0.   0.   0.  ]\n",
      " [0.   0.   0.  ]\n",
      " [0.   0.   0.  ]\n",
      " [0.   0.   0.  ]\n",
      " [0.   0.   0.  ]\n",
      " [0.   0.   0.  ]\n",
      " [0.   0.   0.  ]\n",
      " [0.   0.   0.  ]\n",
      " [0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#Fixed 4096\n",
    "\n",
    "num_tests = 10\n",
    "curr_chunk_size = 4096\n",
    "fixed_4096_tests = {}\n",
    "for i in range(1, num_tests + 1):\n",
    "    print(\"\\nTest: \" + str(i) + \" Chunk Size: \" + str(curr_chunk_size))\n",
    "    fixed_4096_tests[curr_chunk_size] = processFoldersbyChunk(folders, curr_chunk_size, curr_chunk_size, curr_chunk_size)\n",
    "    curr_chunk_size += 4096\n",
    "    \n",
    "dedup_ratios_fixed_4096 = processDedup(fixed_4096_tests)\n",
    "throughput_speeds_fixed_4096  = processThroughput(fixed_4096_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de04702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedup Graph\n",
    "dedup_ratios_fixed_4096_df = pd.DataFrame()\n",
    "dedup_ratios_fixed_4096_df['chunk_sizes'] = fixed_4096_tests.keys()\n",
    "for idx, data in enumerate(dedup_ratios_fixed_4096.T):\n",
    "    dedup_ratios_fixed_4096_df[str(folders[idx])] = data\n",
    "dedup_ratios_fixed_4096_g =dedup_ratios_fixed_4096_df.plot(x=\"chunk_sizes\", y=folders)\n",
    "dedup_ratios_fixed_4096_g.set_ylim(0, 0.2)\n",
    "dedup_ratios_fixed_4096_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Deduplication Ratio (%)\", title =\"Deduplication Ratio by Fixed Chunk Size\")\n",
    "plt.show()\n",
    "\n",
    "# Throughput Graph\n",
    "throughput_speeds_fixed_4096_df = pd.DataFrame()\n",
    "throughput_speeds_fixed_4096_df['chunk_sizes'] = fixed_4096_tests.keys()\n",
    "for idx, data in enumerate(throughput_speeds_fixed_4096.T):\n",
    "    throughput_speeds_fixed_4096_df[str(folders[idx])] = data\n",
    "throughput_speeds_fixed_4096_g =throughput_speeds_fixed_4096_df.plot(x=\"chunk_sizes\", y=folders)\n",
    "throughput_speeds_fixed_4096_g.set_ylim(0, 1400)\n",
    "throughput_speeds_fixed_4096_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Throughput (MB/s)\", title =\"Throughput by Fixed Chunk Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2048 Range\n",
    "\n",
    "byte_range = 2048\n",
    "num_tests = 10\n",
    "curr_chunk_size = 4096\n",
    "range_2048_tests = {}\n",
    "for i in range(1, num_tests + 1):\n",
    "    print(\"\\nTest: \" + str(i) + \" Chunk Size: \" + str(curr_chunk_size))\n",
    "    range_2048_tests[curr_chunk_size] = processFoldersbyChunk(folders, curr_chunk_size - byte_range, curr_chunk_size, curr_chunk_size + byte_range)\n",
    "    curr_chunk_size += 4096\n",
    "    \n",
    "dedup_ratios_range_2048 = processDedup(range_2048_tests)\n",
    "throughput_speeds_range_2048  = processThroughput(range_2048_tests)\n",
    "#print(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69841bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedup Graph\n",
    "dedup_ratios_range_2048_df = pd.DataFrame()\n",
    "dedup_ratios_range_2048_df['chunk_sizes'] = range_2048_tests.keys()\n",
    "for idx, data in enumerate(dedup_ratios_range_2048.T):\n",
    "    dedup_ratios_range_2048_df[str(folders[idx])] = data\n",
    "dedup_ratios_range_2048_g =dedup_ratios_range_2048_df.plot(x=\"chunk_sizes\", y=folders)\n",
    "dedup_ratios_range_2048_g.set_ylim(0, 30)\n",
    "dedup_ratios_range_2048_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Deduplication Ratio (%)\", title =\"Deduplication Ratio by Ranged Chunk Size: 2048 Bytes\")\n",
    "plt.show()\n",
    "\n",
    "# Throughput Graph\n",
    "throughput_speeds_range_2048_df = pd.DataFrame()\n",
    "throughput_speeds_range_2048_df['chunk_sizes'] = range_2048_tests.keys()\n",
    "for idx, data in enumerate(throughput_speeds_range_2048.T):\n",
    "    throughput_speeds_range_2048_df[str(folders[idx])] = data\n",
    "throughput_speeds_range_2048_g =throughput_speeds_range_2048_df.plot(x=\"chunk_sizes\", y=folders)\n",
    "throughput_speeds_range_2048_g.set_ylim(0, 1400)\n",
    "throughput_speeds_range_2048_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Throughput (MB/s)\", title =\"Throughput by Ranged Chunk Size: 2048 Bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4096 Range\n",
    "\n",
    "byte_range = 4096\n",
    "num_tests = 10\n",
    "curr_chunk_size = 8192\n",
    "range_4096_tests = {}\n",
    "for i in range(1, num_tests + 1):\n",
    "    print(\"\\nTest: \" + str(i) + \" Chunk Size: \" + str(curr_chunk_size))\n",
    "    range_4096_tests[curr_chunk_size] = processFoldersbyChunk(folders, curr_chunk_size - byte_range, curr_chunk_size, curr_chunk_size + byte_range)\n",
    "    curr_chunk_size += 4096\n",
    "    \n",
    "dedup_ratios_range_4096 = processDedup(range_4096_tests)\n",
    "throughput_speeds_range_4096  = processThroughput(range_4096_tests)\n",
    "#print(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedup Graph\n",
    "dedup_ratios_range_4096_df = pd.DataFrame()\n",
    "dedup_ratios_range_4096_df['chunk_sizes'] = range_4096_tests.keys()\n",
    "for idx, data in enumerate(dedup_ratios_range_4096.T):\n",
    "    dedup_ratios_range_4096_df[str(folders[idx])] = data\n",
    "dedup_ratios_range_4096_g =dedup_ratios_range_4096_df.plot(x=\"chunk_sizes\", y=folders)\n",
    "dedup_ratios_range_4096_g.set_ylim(0, 25)\n",
    "dedup_ratios_range_4096_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Deduplication Ratio (%)\", title =\"Deduplication Ratio by Ranged Chunk Size: 4096 Bytes\")\n",
    "plt.show()\n",
    "\n",
    "# Throughput Graph\n",
    "throughput_speeds_range_4096_df = pd.DataFrame()\n",
    "throughput_speeds_range_4096_df['chunk_sizes'] = range_4096_tests.keys()\n",
    "for idx, data in enumerate(throughput_speeds_range_4096.T):\n",
    "    throughput_speeds_range_4096_df[str(folders[idx])] = data\n",
    "throughput_speeds_range_4096_g =throughput_speeds_range_4096_df.plot(x=\"chunk_sizes\", y=folders)\n",
    "throughput_speeds_range_4096_g.set_ylim(0, 1400)\n",
    "throughput_speeds_range_4096_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Throughput (MB/s)\", title =\"Throughput by Ranged Chunk Size: 4096 Bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8192 Range\n",
    "\n",
    "byte_range = 8192\n",
    "num_tests = 10\n",
    "curr_chunk_size = 12288\n",
    "range_8192_tests = {}\n",
    "for i in range(1, num_tests + 1):\n",
    "    print(\"\\nTest: \" + str(i) + \" Chunk Size: \" + str(curr_chunk_size))\n",
    "    range_8192_tests[curr_chunk_size] = processFoldersbyChunk(folders, curr_chunk_size - byte_range, curr_chunk_size, curr_chunk_size + byte_range)\n",
    "    curr_chunk_size += 4096\n",
    "    \n",
    "dedup_ratios_range_8192 = processDedup(range_8192_tests)\n",
    "throughput_speeds_range_8192  = processThroughput(range_8192_tests)\n",
    "#print(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d29744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedup Graph\n",
    "dedup_ratios_range_8192_df = pd.DataFrame()\n",
    "dedup_ratios_range_8192_df['chunk_sizes'] = range_8192_tests.keys()\n",
    "for idx, data in enumerate(dedup_ratios_range_8192.T):\n",
    "    dedup_ratios_range_8192_df[str(folders[idx])] = data\n",
    "dedup_ratios_range_8192_g =dedup_ratios_range_8192_df.plot(x=\"chunk_sizes\", y=folders)\n",
    "dedup_ratios_range_8192_g.set_ylim(0, 20)\n",
    "dedup_ratios_range_8192_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Deduplication Ratio (%)\", title =\"Deduplication Ratio by Ranged Chunk Size: 8192 Bytes\")\n",
    "plt.show()\n",
    "\n",
    "# Throughput Graph\n",
    "throughput_speeds_range_8192_df = pd.DataFrame()\n",
    "throughput_speeds_range_8192_df['chunk_sizes'] = range_8192_tests.keys()\n",
    "for idx, data in enumerate(throughput_speeds_range_8192.T):\n",
    "    throughput_speeds_range_8192_df[str(folders[idx])] = data\n",
    "throughput_speeds_range_8192_g =throughput_speeds_range_8192_df.plot(x=\"chunk_sizes\", y=folders)\n",
    "throughput_speeds_range_8192_g.set_ylim(0, 1400)\n",
    "throughput_speeds_range_8192_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Throughput (MB/s)\", title =\"Throughput by Ranged Chunk Size: 8192 Bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    print(folder)\n",
    "    dedup_ratios_all_clean_df = pd.DataFrame()\n",
    "    dedup_ratios_all_clean_df['chunk_sizes'] = dedup_ratios_fixed_4096_df['chunk_sizes']\n",
    "    dedup_ratios_all_clean_df['fixed'] = dedup_ratios_fixed_4096_df[folder]\n",
    "    dedup_ratios_all_clean_df['2048'] = dedup_ratios_range_2048_df[folder]\n",
    "    dedup_ratios_all_clean_df['4096'] = dedup_ratios_range_4096_df[folder]\n",
    "    dedup_ratios_all_clean_df['8192'] = dedup_ratios_range_8192_df[folder]\n",
    "    \n",
    "    range_min = min(dedup_ratios_all_clean_df['fixed'].min(), dedup_ratios_all_clean_df['2048'].min(), dedup_ratios_all_clean_df['4096'].min(), dedup_ratios_all_clean_df['8192'].min())\n",
    "    range_max = max(dedup_ratios_all_clean_df['fixed'].max(), dedup_ratios_all_clean_df['2048'].max(), dedup_ratios_all_clean_df['4096'].max(), dedup_ratios_all_clean_df['8192'].max())\n",
    "\n",
    "    dedup_ratios_all_clean_g =dedup_ratios_all_clean_df.plot(x=\"chunk_sizes\", y=[\"fixed\", \"2048\", \"4096\", \"8192\"])\n",
    "    dedup_ratios_all_clean_g.set_ylim(max(0, range_min - (range_min * 0.2)), range_max * 1.2)\n",
    "    dedup_ratios_all_clean_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Deduplication Ratio (%)\", title =\"Deduplication Ratio by Ranged Chunk Sizes\")\n",
    "    plt.show()\n",
    "\n",
    "    throughput_speeds_all_df = pd.DataFrame()\n",
    "    throughput_speeds_all_df['chunk_sizes'] = throughput_speeds_fixed_4096_df['chunk_sizes']\n",
    "    throughput_speeds_all_df['fixed'] = throughput_speeds_fixed_4096_df[folder]\n",
    "    throughput_speeds_all_df['2048'] = throughput_speeds_range_2048_df[folder]\n",
    "    throughput_speeds_all_df['4096'] = throughput_speeds_range_4096_df[folder]\n",
    "    throughput_speeds_all_df['8192'] = throughput_speeds_range_8192_df[folder]\n",
    "    \n",
    "    range_min = min(throughput_speeds_all_df['fixed'].min(), throughput_speeds_all_df['2048'].min(), throughput_speeds_all_df['4096'].min(), throughput_speeds_all_df['8192'].min())\n",
    "    range_max = max(throughput_speeds_all_df['fixed'].max(), throughput_speeds_all_df['2048'].max(), throughput_speeds_all_df['4096'].max(), throughput_speeds_all_df['8192'].max())\n",
    "\n",
    "    throughput_speeds_all_g =throughput_speeds_all_df.plot(x=\"chunk_sizes\", y=[\"fixed\", \"2048\", \"4096\", \"8192\"])\n",
    "    throughput_speeds_all_g.set_ylim(max(0, range_min - 50), range_max + 50)\n",
    "    throughput_speeds_all_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Throughput (MB/s)\", title =\"Throughput by Ranged Chunk Sizes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be22ffd1",
   "metadata": {},
   "source": [
    "## Memory Dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958d369",
   "metadata": {},
   "source": [
    "### C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44135b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elftools.elf.elffile import ELFFile\n",
    "\n",
    "memorydump_files = []\n",
    "obj = os.scandir(base_path + '/memorydumps/initial/C')\n",
    "print(\"Files and Directories in '% s':\" % (base_path + '/memorydumps/initial/C'))\n",
    "for entry in obj :\n",
    "    if not entry.is_dir() or entry.is_file():\n",
    "        memorydump_files.append(entry.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in memorydump_files:\n",
    "    with open(base_linux + f'/memorydumps/initial/C/{file}', 'rb') as f:\n",
    "        print(f\"Processing File: {file}\")\n",
    "        \n",
    "        # Create an ELFFile object\n",
    "        elf = ELFFile(f)\n",
    "\n",
    "        # Read the ELF header\n",
    "        print('ELF Type:', elf.header['e_type'])\n",
    "        print('Machine Architecture:', elf.header['e_machine'])\n",
    "\n",
    "        # Read the section header table\n",
    "        sh_table = elf.get_section_by_name('.shstrtab')\n",
    "        for section in elf.iter_sections():\n",
    "            print('Section:', section.name)\n",
    "            print('Size:', section['sh_size'])\n",
    "            print('Address:', hex(section['sh_addr']))\n",
    "        print(\"End Of File \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26414c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_zero(bytes_read): \n",
    "    for byte in bytes_read:\n",
    "        if not byte == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in memorydump_files:\n",
    "    with open(base_linux + f'/memorydumps/initial/C/{file}', 'rb') as f:\n",
    "        print(f\"Processing File: {file}\")\n",
    "        \n",
    "        all_zero_pages_count = 0\n",
    "        total_pages = 0\n",
    "        \n",
    "        with open(base_linux + f'/memorydumps/clean/C/{file}_2', 'wb') as output_file:\n",
    "            \n",
    "            while (bytes_read := f.read(4096)):\n",
    "                total_pages += 1\n",
    "\n",
    "                if not bytes:\n",
    "                    break\n",
    "\n",
    "                all_zeros = all_zero(bytes_read)\n",
    "                if all_zeros == True:\n",
    "\n",
    "                    modified_data = bytes_read.replace(b'\\x00', b'')\n",
    "                    output_file.write(modified_data)\n",
    "\n",
    "                    all_zero_pages_count += 1\n",
    "                else:\n",
    "                    output_file.write(bytes_read)\n",
    "            \n",
    "        print(f\"Found {all_zero_pages_count} all-zero pages\")  \n",
    "        print(f\"Found {total_pages} total pages\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56909dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for all-zero pages\n",
    "for file in memorydump_files:\n",
    "    with open(base_linux + f'/memorydumps/clean/C/{file}_2', 'rb') as f:\n",
    "        print(f\"Processing File: {file}\")\n",
    "        \n",
    "        all_zero_pages_count = 0\n",
    "        total_pages = 0\n",
    "        \n",
    "       \n",
    "        while (bytes_read := f.read(4096)):\n",
    "            total_pages += 1\n",
    "\n",
    "            if not bytes:\n",
    "                break\n",
    "\n",
    "            all_zeros = all_zero(bytes_read)\n",
    "            if all_zeros == True:\n",
    "                all_zero_pages_count += 1\n",
    "            \n",
    "        print(f\"Found {all_zero_pages_count} all-zero pages\")  \n",
    "        print(f\"Found {total_pages} total pages\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19545cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Dump Range 2048\n",
    "folders = ['/memorydumps/initial/C', '/memorydumps/clean/C']\n",
    "\n",
    "range_bytes = 2048\n",
    "num_tests = 10\n",
    "curr_chunk_size = 4096\n",
    "memory_range_2048_tests = {}\n",
    "for i in range(1, num_tests + 1):\n",
    "    print(\"\\nTest: \" + str(i) + \" Chunk Size: \" + str(curr_chunk_size))\n",
    "    memory_range_2048_tests[curr_chunk_size] = processFoldersbyChunk(folders, curr_chunk_size - range_bytes, curr_chunk_size, curr_chunk_size + range_bytes)\n",
    "    curr_chunk_size += 4096\n",
    "    \n",
    "dedup_ratios_memory_range_2048 = processDedup(memory_range_2048_tests)\n",
    "throughput_speeds_memory_range_2048  = processThroughput(memory_range_2048_tests)\n",
    "#print(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedup Graph\n",
    "dedup_ratios_memory_range_2048_df = pd.DataFrame()\n",
    "dedup_ratios_memory_range_2048_df['chunk_sizes'] = memory_range_2048_tests.keys()\n",
    "for idx, data in enumerate(dedup_ratios_memory_range_2048.T):\n",
    "    dedup_ratios_memory_range_2048_df[str(folders[idx])] = data\n",
    "dedup_ratios_memory_range_2048_g =dedup_ratios_memory_range_2048_df.plot(x=\"chunk_sizes\", y=[\"/memorydumps/initial/C\", \"/memorydumps/clean/C\"])\n",
    "dedup_ratios_memory_range_2048_g.set_ylim(0, 6)\n",
    "dedup_ratios_memory_range_2048_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Deduplication Ratio (%)\", title =\"Deduplication Ratio by Ranged Chunk Size: 2048 Bytes\")\n",
    "plt.show()\n",
    "\n",
    "# Throughput Graph\n",
    "throughput_speeds_memory_range_2048_df = pd.DataFrame()\n",
    "throughput_speeds_memory_range_2048_df['chunk_sizes'] = memory_range_2048_tests.keys()\n",
    "for idx, data in enumerate(throughput_speeds_memory_range_2048.T):\n",
    "    throughput_speeds_memory_range_2048_df[str(folders[idx])] = data\n",
    "throughput_speeds_memory_range_2048_g =throughput_speeds_memory_range_2048_df.plot(x=\"chunk_sizes\", y=[\"/memorydumps/initial/C\", \"/memorydumps/clean/C\"])\n",
    "throughput_speeds_memory_range_2048_g.set_ylim(0, 1400)\n",
    "throughput_speeds_memory_range_2048_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Throughput (MB/s)\", title =\"Throughput by Ranged Chunk Size: 2048 Bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f170fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Dump Range 4096\n",
    "folders = ['/memorydumps/initial/C', '/memorydumps/clean/C']\n",
    "\n",
    "range_bytes = 4096\n",
    "num_tests = 10\n",
    "curr_chunk_size = 8192\n",
    "memory_range_4096_tests = {}\n",
    "for i in range(1, num_tests + 1):\n",
    "    print(\"\\nTest: \" + str(i) + \" Chunk Size: \" + str(curr_chunk_size))\n",
    "    memory_range_4096_tests[curr_chunk_size] = processFoldersbyChunk(folders, curr_chunk_size - range_bytes, curr_chunk_size, curr_chunk_size + range_bytes)\n",
    "    curr_chunk_size += 4096\n",
    "    \n",
    "dedup_ratios_memory_range_4096 = processDedup(memory_range_4096_tests)\n",
    "throughput_speeds_memory_range_4096  = processThroughput(memory_range_4096_tests)\n",
    "#print(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedup Graph\n",
    "dedup_ratios_memory_range_4096_df = pd.DataFrame()\n",
    "dedup_ratios_memory_range_4096_df['chunk_sizes'] = memory_range_4096_tests.keys()\n",
    "for idx, data in enumerate(dedup_ratios_memory_range_4096.T):\n",
    "    dedup_ratios_memory_range_4096_df[str(folders[idx])] = data\n",
    "dedup_ratios_memory_range_4096_g =dedup_ratios_memory_range_4096_df.plot(x=\"chunk_sizes\", y=[\"/memorydumps/initial/C\", \"/memorydumps/clean/C\"])\n",
    "dedup_ratios_memory_range_4096_g.set_ylim(0, 6)\n",
    "dedup_ratios_memory_range_4096_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Deduplication Ratio (%)\", title =\"Deduplication Ratio by Ranged Chunk Size: 4096 Bytes\")\n",
    "plt.show()\n",
    "\n",
    "# Throughput Graph\n",
    "throughput_speeds_memory_range_4096_df = pd.DataFrame()\n",
    "throughput_speeds_memory_range_4096_df['chunk_sizes'] = memory_range_4096_tests.keys()\n",
    "for idx, data in enumerate(throughput_speeds_memory_range_4096.T):\n",
    "    throughput_speeds_memory_range_4096_df[str(folders[idx])] = data\n",
    "throughput_speeds_memory_range_4096_g =throughput_speeds_memory_range_4096_df.plot(x=\"chunk_sizes\", y=[\"/memorydumps/initial/C\", \"/memorydumps/clean/C\"])\n",
    "throughput_speeds_memory_range_4096_g.set_ylim(0, 1400)\n",
    "throughput_speeds_memory_range_4096_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Throughput (MB/s)\", title =\"Throughput by Ranged Chunk Size: 4096 Bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Dump Range 8192\n",
    "folders = ['/memorydumps/initial/C', '/memorydumps/clean/C']\n",
    "\n",
    "range_bytes = 8192\n",
    "num_tests = 10\n",
    "curr_chunk_size = 12288\n",
    "memory_range_8192_tests = {}\n",
    "for i in range(1, num_tests + 1):\n",
    "    print(\"\\nTest: \" + str(i) + \" Chunk Size: \" + str(curr_chunk_size))\n",
    "    memory_range_8192_tests[curr_chunk_size] = processFoldersbyChunk(folders, curr_chunk_size - range_bytes, curr_chunk_size, curr_chunk_size + range_bytes)\n",
    "    curr_chunk_size += 4096\n",
    "    \n",
    "dedup_ratios_memory_range_8192 = processDedup(memory_range_8192_tests)\n",
    "throughput_speeds_memory_range_8192  = processThroughput(memory_range_8192_tests)\n",
    "#print(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb766170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedup Graph\n",
    "dedup_ratios_memory_range_8192_df = pd.DataFrame()\n",
    "dedup_ratios_memory_range_8192_df['chunk_sizes'] = memory_range_8192_tests.keys()\n",
    "for idx, data in enumerate(dedup_ratios_memory_range_8192.T):\n",
    "    dedup_ratios_memory_range_8192_df[str(folders[idx])] = data\n",
    "dedup_ratios_memory_range_8192_g =dedup_ratios_memory_range_8192_df.plot(x=\"chunk_sizes\", y=[\"/memorydumps/initial/C\", \"/memorydumps/clean/C\"])\n",
    "dedup_ratios_memory_range_8192_g.set_ylim(0, 6)\n",
    "dedup_ratios_memory_range_8192_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Deduplication Ratio (%)\", title =\"Deduplication Ratio by Ranged Chunk Size: 8192 Bytes\")\n",
    "plt.show()\n",
    "\n",
    "# Throughput Graph\n",
    "throughput_speeds_memory_range_8192_df = pd.DataFrame()\n",
    "throughput_speeds_memory_range_8192_df['chunk_sizes'] = memory_range_8192_tests.keys()\n",
    "for idx, data in enumerate(throughput_speeds_memory_range_8192.T):\n",
    "    throughput_speeds_memory_range_8192_df[str(folders[idx])] = data\n",
    "throughput_speeds_memory_range_8192_g =throughput_speeds_memory_range_8192_df.plot(x=\"chunk_sizes\", y=[\"/memorydumps/initial/C\", \"/memorydumps/clean/C\"])\n",
    "throughput_speeds_memory_range_8192_g.set_ylim(0, 1400)\n",
    "throughput_speeds_memory_range_8192_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Throughput (MB/s)\", title =\"Throughput by Ranged Chunk Size: 8192 Bytes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67728343",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_ratios_memory_range_all_clean_df = pd.DataFrame()\n",
    "dedup_ratios_memory_range_all_clean_df['chunk_sizes'] = dedup_ratios_memory_range_2048_df['chunk_sizes']\n",
    "dedup_ratios_memory_range_all_clean_df['2048'] = dedup_ratios_memory_range_2048_df['/memorydumps/clean/C']\n",
    "dedup_ratios_memory_range_all_clean_df['4096'] = dedup_ratios_memory_range_4096_df['/memorydumps/clean/C']\n",
    "dedup_ratios_memory_range_all_clean_df['8192'] = dedup_ratios_memory_range_8192_df['/memorydumps/clean/C']\n",
    "\n",
    "dedup_ratios_memory_range_all_clean_g =dedup_ratios_memory_range_all_clean_df.plot(x=\"chunk_sizes\", y=[\"2048\", \"4096\", \"8192\"])\n",
    "dedup_ratios_memory_range_all_clean_g.set_ylim(1.5, 2)\n",
    "dedup_ratios_memory_range_all_clean_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Deduplication Ratio (%)\", title =\"Deduplication Ratio by Ranged Chunk Sizes\")\n",
    "plt.show()\n",
    "\n",
    "dedup_ratios_memory_range_all_initial_df = pd.DataFrame()\n",
    "dedup_ratios_memory_range_all_initial_df['chunk_sizes'] = throughput_speeds_memory_range_2048_df['chunk_sizes']\n",
    "dedup_ratios_memory_range_all_initial_df['2048'] = throughput_speeds_memory_range_2048_df['/memorydumps/clean/C']\n",
    "dedup_ratios_memory_range_all_initial_df['4096'] = throughput_speeds_memory_range_4096_df['/memorydumps/clean/C']\n",
    "dedup_ratios_memory_range_all_initial_df['8192'] = throughput_speeds_memory_range_8192_df['/memorydumps/clean/C']\n",
    "\n",
    "dedup_ratios_memory_range_all_initial_g =dedup_ratios_memory_range_all_initial_df.plot(x=\"chunk_sizes\", y=[\"2048\", \"4096\", \"8192\"])\n",
    "dedup_ratios_memory_range_all_initial_g.set_ylim(500, 1300)\n",
    "dedup_ratios_memory_range_all_initial_g.set(xlabel =\"Chuck Sizes (Bytes)\", ylabel = \"Throughput (MB/s)\", title =\"Throughput by Ranged Chunk Sizes\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
